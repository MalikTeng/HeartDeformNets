# HeartDeformNets

This repository forks from the [fkong/HeartDeformNet](https://github.com/fkong7/HeartDeformNets.git) used for their paper:

Fanwei Kong, Shawn Shadden, Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations (2022)

<img width="1961" alt="network2" src="https://user-images.githubusercontent.com/31931939/184846001-eb3b9442-ae46-4152-a3dc-791e1ccdf946.png">

## Installation

To download the source code with submodules, please run:
```
git clone --recurse-submodules https://github.com/MalikTeng/HeartDeformNets.git
```
To prepare the environment for this implementation, follow the steps below:
```
cd HeartDeformNets
conda create -n deformnet python=3.9
conda activate deformnet
pip install -r requirements.txt
```

## Template Meshes

_Due to VTK version compatibility issues, the commented-out steps for template creation from segmentation in `create_template.sh` may not work reliably. The current workflow requires a pre-existing template mesh file._

To process a template mesh (e.g., left ventricle myocardium) and generate necessary files for training, follow the steps below:

1.  **Prepare `template.obj`**: Obtain or create a surface mesh of your desired template structure (e.g., left ventricle myocardium) and save it as `template.obj`.
2.  **Compile Biharmonic Coordinates Code**: Compile the C++ code in `templates/bc` if you haven't already:
    ```bash
    cd templates/bc
    mkdir build
    cd build
    cmake ..
    make
    cd ../../.. # Return to the project root directory
    ```
3.  **Run Processing Script**: Run `create_template.sh`. You will be prompted to enter the directory where your `template.obj` is located and where the output files should be saved (`output_dir`). Place your `template.obj` file inside this `output_dir` *before* running the script.
    ```bash
    cd templates # Navigate to the templates directory if not already there
    ./create_template.sh
    cd .. # Return to the project root directory
    ```
    This script will:
    *   Clean the input `template.obj` -> `template_merged.obj`.
    *   Generate a tetrahedral mesh -> `template_manifold.obj`.
    *   Sample control points using Farthest Point Sampling (FPS) for different handle counts (e.g., 75, 600) -> `template_merged_{num_handles}pts.obj`.
    *   Compute Biharmonic Coordinates weights -> `template_{num_handles}pts.csv`.
    *   Adjust control points -> `template_merged_{num_handles}pts_adjusted.obj`.
    *   The script will save the paths to these generated control point and weight files in `ctrl_fns.txt` and `weight_fns.txt` within the `output_dir` for the next step.
4.  **Manual Correction (Important!)**:
    *   The script generates intermediate files like `template_manifold.obj`. Load this `.obj` file (or a `.vtp` file if generated by commented-out steps or other tools) into mesh processing software (e.g., __ParaView__).
    *   Inspect the mesh and correct any errors (e.g., non-manifold edges, intersecting faces).
    *   If using ParaView, ensure you select the "RegionID" data (if available and relevant for your template) when saving.
    *   Save the corrected mesh as `template.vtp` (using __ASCII__ format) in the *same `output_dir`* specified in step 3, overwriting any existing file with that name.
5.  **Generate Training Data File**: Run `create_dat.sh`. You will be prompted to enter the `output_dir` (ensure it's the same one used in step 3). The script will automatically read the control point and weight file paths from the temporary files created by `create_template.sh`.
    ```bash
    cd templates # Navigate to the templates directory if not already there
    chmod +x create_dat.sh
    ./create_dat.sh
    cd .. # Return to the project root directory
    ```
    This script calls `make_mesh_info_dat.py` to generate the final `.dat` file needed for training, using the corrected `template.vtp` and the computed weights/control points.
6.  **Final Check**: Check the output VTP file (named with date and time) generated by `make_mesh_info_dat.py` in the `output_dir`. If necessary, perform another round of manual correction in ParaView as described in step 5.

## Training

### Data Preparation

The data preparation code is copied from the author's [MeshDeformNet](https://github.com/fkong7/MeshDeformNet.git) repository.

Ensure you have a directory structure as follows (e.g., for the __MMWHS__ dataset):
```
|-- MMWHS
    |-- nii
        |-- ct_train
            |-- 01.nii.gz
            |-- 02.nii.gz
            |-- ...
        |-- ct_train_seg
            |-- 01.nii.gz
            |-- 02.nii.gz
            |-- ...
        |-- ct_val
        |-- ct_val_seg
        |-- mr_train
        |-- mr_train_seg
        |-- mr_val
        |-- mr_val_seg
```
_I have all images and labels foreground cropped, resized, and padded to 128x128x128. Not sure what will happen if not doing so._

The data preparation steps are as follows:
- If data augmentation is preferred, you need the __mpi4py__ (Python bindings for MPI) installed. Here is how to install it on a Linux machine.
    - Install MPI implementation:
        ```
        sudo apt update
        sudo apt install libopenmpi-dev
        ```
    - Install mpi4py (if you are in the `deformnet` environment):
        ```
        pip install mpi4py
        ```
- Run data augmentation with the following command (I did this by running on a local machine, but if you are doing this with SSH on a remote machine, you need to install __X11__ and __xauth__):
    ```
    mpirun -n 4 python data/data_augmentation.py \
        --im_dir /path/to/MMWHS/nii/ct_train \
        --seg_dir /path/to/MMWHS/nii/ct_train_seg \
        --out_dir /path/to/MMWHS/nii_augmented \
        --modality ct \ # ct or mr
        --mode train \  # train or val
        --num 10        # number of augmented copies per image, 10 for ct and 20 for mr
    ```

    Note: I used `-n 4` instead of `-n 20` as the author suggested in __MeshDeformNet__ due to limited resources on my local machine.
- Do the same for `val` data if needed. The above command will produce a new folder `nii_augmented` in the `MMWHS` directory.
- Preprocess data by applying intensity normalization and resizing. Results will be saved as TFRecords files in a new folder `tfrecords` in the `MMWHS` directory.
    ```
    python data/data2tfrecords.py \
        --folder /path/to/MMWHS/nii \
        --modality ct \              # ct or mr
        --size 128 128 128 \         # image dimensions for training
        --folder_postfix _train \    # _train or _val, i.e. will process the images/segmentation in ct_train and ct_train_seg
        --deci_rate 0  \             # decimation rate on ground truth surface meshes
        --smooth_ite 50 \            # Laplacian smoothing on ground truth surface meshes
        --out_folder /path/to/MMWHS/tfrecords \
        --seg_id 1                   # segmentation ids, 1-7 for seven cardiac structures
    ```
- Do the same for augmented data in the `nii_augmented` folder as above, if you are using data augmentation.

### Compile nndistance Loss

If you do not see a `tf_nndistance_so.so` file in the `external/` directory, which is a required Python module compiled in C++ for training the network, compile the module by running the following command:
```
cd external/
make
cd ..
```
**Note:** If you encounter compilation errors, you might need to edit the `external/makefile`. Ensure the CUDA and TensorFlow paths are correct for your system. Additionally, depending on your TensorFlow version (e.g., TF 2.10), you might need to change the C++ standard from `