# HeartDeformNets

This repository forks from the [fkong/HeartDeformNet](https://github.com/fkong7/HeartDeformNets.git) used for their paper:

Fanwei Kong, Shawn Shadden, Learning Whole Heart Mesh Generation From Patient Images For Computational Simulations (2022)

<img width="1961" alt="network2" src="https://user-images.githubusercontent.com/31931939/184846001-eb3b9442-ae46-4152-a3dc-791e1ccdf946.png">

## Installation

To download the source code with submodules, please run:
```
git clone --recurse-submodules https://github.com/MalikTeng/HeartDeformNets.git
```
To prepare the environment and install all dependencies, run the installation script:
```bash
cd HeartDeformNets
bash install_packages.sh
```
This script will automatically create (or recreate if it exists) the 'deformnet' Conda environment with Python 3.9, install VTK using Conda, and then install the remaining packages listed in `requirements.txt` using pip.

After the script finishes, activate the environment using:
```bash
conda activate deformnet
```

## Template Meshes

_Due to VTK version compatibility issues, the commented-out steps for template creation from segmentation in `create_template.sh` may not work reliably. The current workflow requires a pre-existing template mesh file._

To process a template mesh (e.g., left ventricle myocardium) and generate necessary files for training, follow the steps below:

1.  **Prepare `template.obj`**: Obtain or create a surface mesh of your desired template structure (e.g., left ventricle myocardium) and save it as `template.obj`.
2.  **Compile Biharmonic Coordinates Code**: Compile the C++ code in `templates/bc` if you haven't already:
    ```bash
    cd templates/bc
    mkdir build
    cd build
    cmake ..
    make
    cd ../../.. # Return to the project root directory
    ```
3.  **Run Processing Script**: Run `create_template.sh`. You will be prompted to enter the directory where your `template.obj` is located and where the output files should be saved (`output_dir`). Place your `template.obj` file inside this `output_dir` *before* running the script.
    ```bash
    cd templates # Navigate to the templates directory if not already there
    ./create_template.sh
    cd .. # Return to the project root directory
    ```
    This script will:
    *   Clean the input `template.obj` -> `template_merged.obj`.
    *   Generate a tetrahedral mesh -> `template_manifold.obj`.
    *   Sample control points using Farthest Point Sampling (FPS) for different handle counts (e.g., 75, 600) -> `template_merged_{num_handles}pts.obj`.
    *   Compute Biharmonic Coordinates weights -> `template_{num_handles}pts.csv`.
    *   Adjust control points -> `template_merged_{num_handles}pts_adjusted.obj`.
    *   The script will save the paths to these generated control point and weight files in `ctrl_fns.txt` and `weight_fns.txt` within the `output_dir` for the next step.
4.  **Manual Correction (Important!)**:
    *   The script generates intermediate files like `template_manifold.obj`. Load this `.obj` file (or a `.vtp` file if generated by commented-out steps or other tools) into mesh processing software (e.g., __ParaView__).
    *   Inspect the mesh and correct any errors (e.g., non-manifold edges, intersecting faces).
    *   If using ParaView, ensure you select the "RegionID" data (if available and relevant for your template) when saving.
    *   Save the corrected mesh as `template.vtp` (using __ASCII__ format) in the *same `output_dir`* specified in step 3, overwriting any existing file with that name.
5.  **Generate Training Data File**: Run `create_dat.sh`. You will be prompted to enter the `output_dir` (ensure it's the same one used in step 3). The script will automatically read the control point and weight file paths from the temporary files created by `create_template.sh`.
    ```bash
    cd templates # Navigate to the templates directory if not already there
    chmod +x create_dat.sh
    ./create_dat.sh
    cd .. # Return to the project root directory
    ```
    This script calls `make_mesh_info_dat.py` to generate the final `.dat` file needed for training, using the corrected `template.vtp` and the computed weights/control points.
6.  **Final Check**: Check the output VTP file (named with date and time) generated by `make_mesh_info_dat.py` in the `output_dir`. If necessary, perform another round of manual correction in ParaView as described in step 5.

## Training

### Data Preparation

The data preparation code is copied from the author's [MeshDeformNet](https://github.com/fkong7/MeshDeformNet.git) repository.

Ensure you have a directory structure as follows (e.g., for the __MMWHS__ dataset):
```
|-- MMWHS
    |-- nii
        |-- ct_train
            |-- 01.nii.gz
            |-- 02.nii.gz
            |-- ...
        |-- ct_train_seg
            |-- 01.nii.gz
            |-- 02.nii.gz
            |-- ...
        |-- ct_val
        |-- ct_val_seg
        |-- mr_train
        |-- mr_train_seg
        |-- mr_val
        |-- mr_val_seg
```
_I have all images and labels foreground cropped, resized, and padded to 128x128x128. Not sure what will happen if not doing so._

The data preparation steps are as follows:
- If data augmentation is preferred, you need the __mpi4py__ (Python bindings for MPI) installed. Here is how to install it on a Linux machine.
    - Install MPI implementation:
        ```
        sudo apt update
        sudo apt install libopenmpi-dev
        ```
    - Install mpi4py (if you are in the `deformnet` environment):
        ```
        pip install mpi4py
        ```
- Run data augmentation with the following command (I did this by running on a local machine, but if you are doing this with SSH on a remote machine, you need to install __X11__ and __xauth__):
    ```
    mpirun -n 4 python data/data_augmentation.py \
        --im_dir /path/to/MMWHS/nii/ct_train \
        --seg_dir /path/to/MMWHS/nii/ct_train_seg \
        --out_dir /path/to/MMWHS/nii_augmented \
        --modality ct \ # ct or mr
        --mode train \  # train or val
        --num 10        # number of augmented copies per image, 10 for ct and 20 for mr
    ```

    Note: I used `-n 4` instead of `-n 20` as the author suggested in __MeshDeformNet__ due to limited resources on my local machine.
- Do the same for `val` data if needed. The above command will produce a new folder `nii_augmented` in the `MMWHS` directory.
- Preprocess data by applying intensity normalization and resizing. Results will be saved as TFRecords files in a new folder `tfrecords` in the `MMWHS` directory.
    ```
    python data/data2tfrecords.py \
        --folder /path/to/MMWHS/nii \
        --modality ct \              # ct or mr
        --size 128 128 128 \         # image dimensions for training
        --folder_postfix _train \    # _train or _val, i.e. will process the images/segmentation in ct_train and ct_train_seg
        --deci_rate 0  \             # decimation rate on ground truth surface meshes
        --smooth_ite 50 \            # Laplacian smoothing on ground truth surface meshes
        --out_folder /path/to/MMWHS/tfrecords \
        --seg_id 1                   # segmentation ids, 1-7 for seven cardiac structures
    ```
- Do the same for augmented data in the `nii_augmented` folder as above, if you are using data augmentation.

### Compile nndistance Loss

If you do not see a `tf_nndistance_so.so` file in the `external/` directory, which is a required Python module compiled in C++ for training the network, compile the module by running the following command:
```
cd external/
make
cd ..
```
**Note:** If you encounter compilation errors, you might need to edit the `external/makefile`. Ensure the CUDA and TensorFlow paths are correct for your system. Additionally, depending on your TensorFlow version (e.g., TF 2.10), you might need to change the C++ standard from `c++11` to `c++14` or `c++17`.

### Running Training

Once the data is prepared and the necessary modules are compiled:

1.  **Configure Training**: Modify one of the configuration files in the `config/` directory (e.g., `config/task1_mmwhs.yaml`) or create your own. Ensure the paths to your datasets, template `.dat` file, and output directories are correctly specified. Pay close attention to parameters like `mesh_dat_filemame`, `train_img_folder`, `val_img_folder`, `output_folder`, etc.
2.  **Start Training**: Run the training script with the chosen configuration file:
    ```bash
    python train.py --config config/your_config_file.yaml
    ```
    Replace `your_config_file.yaml` with the actual name of your configuration file. The script will load the parameters from the YAML file and start the training process. Model weights will be saved periodically (based on validation performance) to the specified `output_folder` in the config file (e.g., `weights_gcn.hdf5`).

## Prediction

After training a model, you can use it to predict mesh deformations on new images.

1.  **Configure Prediction**: Modify one of the configuration files in the `config/` directory or create a new one specifically for prediction. Ensure the paths to the input images (`image_folder`), the trained model weights (`model_weights_filename`), the template mesh (`mesh_tmplt_filename`), the template `.dat` file (`mesh_dat_filemame`), and the output directory (`output_folder`) are correctly specified.
2.  **Run Prediction**: Execute the prediction script with the appropriate configuration file:
    ```bash
    python predict.py --config config/your_prediction_config.yaml
    ```
    Replace `your_prediction_config.yaml` with your prediction configuration file. The script will load the images, apply the trained model, and save the resulting deformed meshes (usually as `.vtp` files) and optionally segmentation masks (e.g., `.nii.gz`) to the specified `output_folder`.

Example configuration files (`cap.yaml`, `scotheart.yaml`, `task1_mmwhs.yaml`) are provided in the `config/` directory as starting points.